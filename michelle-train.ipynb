{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel, LabeledPoint, RandomForest\n",
    "\n",
    "app_name = \"w261_michelle_training\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawRDD = spark.read.csv(\"data/sample.txt\", header=False, sep=\"\\t\").rdd\n",
    "dataRDD = rawRDD.map(lambda row: ([None if el is None else int(el) for el in row[1:14]] + list(row[14:]), int(row[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data we saved from the EDA. This helps us engineer the features and configure the model\n",
    "\n",
    "frequent_feats = {}\n",
    "\n",
    "with open(\"data/freq_category_counts.csv\") as csvfile:\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        total = int(row[\"total\"])\n",
    "        if total >= 10:\n",
    "            key = \"{}-{}\".format(row[\"col_name\"], row[\"category\"])\n",
    "            frequent_feats[key] = int(row[\"category_id\"])\n",
    "\n",
    "with open(\"data/num_significant_categories.csv\") as csvfile:\n",
    "    num_significant_categories = { row[\"field\"]: int(row[\"count\"]) for row in csv.DictReader(csvfile) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles of Trees\n",
    "\n",
    "The numerical columns are pretty much used directly. Note that NULLs are encoded with the value `-10` (also tried 0 and imputing with the medians). Categorical values are kept if they are in the common categories from the EDA. They were assigned an integer ID, which is in the `frequent_feats` dict. This is used to encode each value. All rare values are converted to a special ID. NULLs are converted to yet another special ID. Note that this adds 2 additional categories from the ones we picked from the EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labeled(pair):\n",
    "    \"\"\"transform input data into the features\"\"\"\n",
    "    row, label = pair\n",
    "    # collect the converted values here\n",
    "    vector = []\n",
    "    \n",
    "    for i, val in enumerate(row):\n",
    "        # if this is an numerical column\n",
    "        if i < 13:\n",
    "            if val is None:\n",
    "                val = -10\n",
    "            vector.append(val)\n",
    "    return LabeledPoint(label, vector)\n",
    "\n",
    "def resample(pair):\n",
    "    \"\"\"sample the positive examples twice to increase their importance\"\"\"\n",
    "    if pair.label == 1:\n",
    "        return [pair, pair]\n",
    "    else:\n",
    "        return [pair]\n",
    "\n",
    "labeledRDD = dataRDD.map(to_labeled)\n",
    "\n",
    "trainingData, validationData = labeledRDD.randomSplit([0.9, 0.1])\n",
    "# re-samples the positive class\n",
    "#trainingData = trainingData.flatMap(resample)\n",
    "\n",
    "labels = validationData.map(lambda lp: lp.label).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [1.0,0.0,66.0,7.0,230.0,37.0,11.0,39.0,58.0,1.0,2.0,-10.0,28.0]),\n",
       " LabeledPoint(0.0, [8.0,2.0,8.0,4.0,0.0,0.0,25.0,9.0,175.0,2.0,5.0,-10.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,1.0,1.0,4829.0,47.0,2.0,5.0,38.0,0.0,1.0,-10.0,1.0]),\n",
       " LabeledPoint(1.0, [2.0,4.0,-10.0,1.0,40.0,1.0,2.0,1.0,1.0,1.0,1.0,-10.0,1.0]),\n",
       " LabeledPoint(1.0, [-10.0,1.0,-10.0,1.0,6916.0,28.0,4.0,4.0,31.0,-10.0,2.0,-10.0,1.0])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7562296858071506\n"
     ]
    }
   ],
   "source": [
    "model_gbdt = GradientBoostedTrees.trainClassifier(trainingData, maxBins=13,\n",
    "                                                 categoricalFeaturesInfo={}, maxDepth=6, numIterations=10)\n",
    "\n",
    "# Evaluate model on test instances and compute validation accuracy\n",
    "predictions_gbdt = model_gbdt.predict(validationData.map(lambda x: x.features))\n",
    "#labelsAndPredictions = validationData.map(lambda lp: lp.label).zip(predictions)\n",
    "#testErr = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(validationData.count())\n",
    "\n",
    "preds_gbdt = predictions_gbdt.collect()\n",
    "accuracy_gbdt = np.mean(np.array(labels) == np.array(preds_gbdt))\n",
    "print('accuracy = ' + str(accuracy_gbdt))\n",
    "\n",
    "model_name = \"models/gbdt-model\"\n",
    "# save the model\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_gbdt.save(sc, model_name)\n",
    "\n",
    "# how to load the model again, tho not necessary in this file\n",
    "#sameModel = GradientBoostedTreesModel.load(sc, model_name)\n",
    "#print(sameModel.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "gb_grid_params = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "              'max_depth': [4, 6, 8],\n",
    "              'min_samples_leaf': [20, 50,100,150],\n",
    "              #'max_features': [1.0, 0.3, 0.1] \n",
    "              }\n",
    "print(gb_grid_params)\n",
    "\n",
    "gb_gs = GradientBoostingClassifier(n_estimators = 600)\n",
    "\n",
    "clf = grid_search.GridSearchCV(gb_gs,\n",
    "                               gb_grid_params,\n",
    "                               cv=2,\n",
    "                               scoring='roc_auc',\n",
    "                               verbose = 3, \n",
    "                               n_jobs=10);\n",
    "clf.fit(train_gs_X, train_gs_Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.95      0.85      7589\n",
      "        1.0       0.55      0.19      0.28      2564\n",
      "\n",
      "avg / total       0.72      0.76      0.71     10153\n",
      "\n",
      "[[7196  393]\n",
      " [2082  482]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_gbdt))\n",
    "print(confusion_matrix(labels, preds_gbdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some config I tried so far:\n",
    "\n",
    "|max depth|num iterations|top n categories|re-sampling ratio neg:pos|acc|1-recall|F1 score|note|\n",
    "|---------|--------------|----------------|-------------------------|---|--------|--|----|\n",
    "|3|5|200000|1:1|-|-|-|the model fails to train due to out-of-memory error|\n",
    "|3|5|50|1:1|75%|9%|67%|basically predicts all 0's|\n",
    "|6|10|50|1:1|76%|21%|71%|Model getting much bigger though. Takes about 2.5 hours to train full data on my laptop|\n",
    "|6|10|50|1:2|72%|50%|73%|trades off accuracy for recall|\n",
    "|6|15|50|1:2|72%|50%|73%|no improvement|\n",
    "|12|10|50|1:2|70%|48%|71%|actually gets worse on all metrics|\n",
    "|6|10|75|1:2|72%|50%|73%|no improvement|\n",
    "|4|10|75|1:2|71%|49%|72%|slight decrease on all metrics|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7517037037037037\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForest.trainClassifier(trainingData,\n",
    "                                        categoricalFeaturesInfo=categoricalFeaturesInfo,\n",
    "                                        maxBins=maxBins,\n",
    "                                        numClasses=2,\n",
    "                                        maxDepth=15,\n",
    "                                        numTrees=10)\n",
    "\n",
    "# Evaluate model on test instances and compute validation accuracy\n",
    "predictions_rf = model_rf.predict(validationData.map(lambda x: x.features))\n",
    "\n",
    "preds_rf = predictions_rf.collect()\n",
    "accuracy_rf = np.mean(np.array(labels) == np.array(preds_rf))\n",
    "print('accuracy = ' + str(accuracy_rf))\n",
    "\n",
    "model_name = \"models/rf-model\"\n",
    "# save the model\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_rf.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.91      0.84      7511\n",
      "        1.0       0.54      0.29      0.38      2614\n",
      "\n",
      "avg / total       0.72      0.75      0.72     10125\n",
      "\n",
      "[[6852  659]\n",
      " [1855  759]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_rf))\n",
    "print(confusion_matrix(labels, preds_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some config I tried so far:\n",
    "\n",
    "|max depth|num trees|top n categories|re-sampling ratio neg:pos|acc|1-recall|F1 score|note|\n",
    "|---------|---------|----------------|-------------------------|---|--------|--|----|\n",
    "|6|10|50|1:1|75%|19%|70%|almost as good as GBDT|\n",
    "|10|10|50|1:1|75%|23%|71%|benefits from a little more depth|\n",
    "|10|15|50|1:1|75%|19%|70%|more trees is worse|\n",
    "|15|15|50|1:1|75%|27%|72%|but more depth continues to benefit|\n",
    "|15|10|50|1:1|75%|31%|73%|and dialing back # of trees is even better|\n",
    "|15|5|50|1:1|75%|29%|72%|but too few trees is bad|\n",
    "|30|5|50|1:1|72%|42%|71%|doubling the depth gets very good recall|\n",
    "|30|15|50|1:1|74%|34%|72%|adding more trees shifts the trade-off|\n",
    "|15|10|50|1:2|68%|62%|70%|resampling gets way better recall|\n",
    "|15|15|50|1:2|70%|60%|71%||\n",
    "|20|15|50|1:2|69%|59%|70%||\n",
    "|10|10|50|1:2|70%|55%|71%||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start desperately throwing models at this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_encoded(pair):\n",
    "    \"\"\"transform input data into the features, but use one-hot encoding for the categorical variables\"\"\"\n",
    "    row, label = pair\n",
    "    # collect the converted values here\n",
    "    vector = []\n",
    "    \n",
    "    for i, val in enumerate(row):\n",
    "        # if this is an numerical column\n",
    "        if i < 13:\n",
    "            if val is None:\n",
    "                val = 0\n",
    "            val = np.max([0, val])\n",
    "            vector.append(val)\n",
    "        # if this is categorical\n",
    "        else:\n",
    "            feat_len = num_significant_categories[\"C\" + str(i - 13)]\n",
    "            one_hot = np.zeros(feat_len + 2)\n",
    "            if val is not None:\n",
    "                key = \"C{}-{}\".format(i - 13, val)\n",
    "                # if its one of our \"common\" values\n",
    "                if key in frequent_feats:\n",
    "                    # look up its ID\n",
    "                    one_hot[frequent_feats[key]] = 1\n",
    "                else:\n",
    "                    # give it the special value for RARE\n",
    "                    one_hot[feat_len] = 1\n",
    "            else:\n",
    "                # give it the special value for NULL\n",
    "                one_hot[feat_len + 1] = 1\n",
    "            vector.extend(one_hot)\n",
    "    return LabeledPoint(label, vector)\n",
    "\n",
    "def resample(pair):\n",
    "    \"\"\"sample the positive examples twice to increase their importance\"\"\"\n",
    "    if pair.label == 1:\n",
    "        return [pair, pair]\n",
    "    else:\n",
    "        return [pair]\n",
    "\n",
    "encodedRDD = dataRDD.map(to_encoded)\n",
    "trainingData, validationData = encodedRDD.randomSplit([0.9, 0.1])\n",
    "labels = validationData.map(lambda lp: lp.label).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.46170527353376045\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model_nb = NaiveBayes.train(trainingData, lambda_=1.0)\n",
    "predictions_nb = model_nb.predict(validationData.map(lambda x: x.features))\n",
    "\n",
    "preds_nb = predictions_nb.collect()\n",
    "accuracy_nb = np.mean(np.array(labels) == np.array(preds_nb))\n",
    "print(\"accuracy = \" + str(accuracy_nb))\n",
    "\n",
    "model_name = \"models/nb-model\"\n",
    "# save the model\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_nb.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.36      0.50      7572\n",
      "        1.0       0.29      0.75      0.42      2573\n",
      "\n",
      "avg / total       0.68      0.46      0.48     10145\n",
      "\n",
      "[[2746 4826]\n",
      " [ 635 1938]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_nb))\n",
    "print(confusion_matrix(labels, preds_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7235091177920158\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "model_lr = LogisticRegressionWithSGD.train(trainingData)\n",
    "predictions_lr = model_lr.predict(validationData.map(lambda x: x.features))\n",
    "\n",
    "preds_lr = predictions_lr.collect()\n",
    "accuracy_lr = np.mean(np.array(labels) == np.array(preds_lr))\n",
    "print(\"accuracy = \" + str(accuracy_lr))\n",
    "\n",
    "model_name = \"models/lr-model\"\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_lr.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.91      0.83      7572\n",
      "        1.0       0.39      0.16      0.23      2573\n",
      "\n",
      "avg / total       0.67      0.72      0.68     10145\n",
      "\n",
      "[[6927  645]\n",
      " [2160  413]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_lr))\n",
    "print(confusion_matrix(labels, preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model_svm = SVMWithSGD.train(trainingData)\n",
    "predictions_svm = model_svm.predict(validationData.map(lambda x: x.features))\n",
    "\n",
    "preds_svm = predictions_svm.collect()\n",
    "accuracy_svm = np.mean(np.array(labels) == np.array(preds_svm))\n",
    "print(\"accuracy = \" + str(accuracy_svm))\n",
    "\n",
    "model_name = \"models/svm-model\"\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_svm.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels, preds_svm))\n",
    "print(confusion_matrix(labels, preds_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
