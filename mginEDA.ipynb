{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting missingno\n",
      "  Downloading https://files.pythonhosted.org/packages/57/eb/9d7d55ceec57e0e374e70e9ad8d16795ba91960a3c987f3b5ee71d3e8e4d/missingno-0.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: seaborn in /opt/anaconda/lib/python3.6/site-packages (from missingno) (0.9.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/lib/python3.6/site-packages (from missingno) (1.15.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda/lib/python3.6/site-packages (from missingno) (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/anaconda/lib/python3.6/site-packages (from missingno) (1.1.0)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /opt/anaconda/lib/python3.6/site-packages (from seaborn->missingno) (0.23.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->missingno) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->missingno) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->missingno) (2.7.3)\n",
      "Requirement already satisfied: pytz in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->missingno) (2018.5)\n",
      "Requirement already satisfied: six>=1.10 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->missingno) (1.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->missingno) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->missingno) (40.2.0)\n",
      "Installing collected packages: missingno\n",
      "Successfully installed missingno-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import chi2_contingency\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"proj_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 1: 27590 Killed                  shuf -n 100000 data/train.txt > data/sample.txt\n"
     ]
    }
   ],
   "source": [
    "# shuf is a linux commands that gets a random permutation of the lines of a file\n",
    "!shuf -n 100000 data/train.txt > data/sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawRDD = spark.read.csv(\"data/sample.txt\", header=False, sep=\"\\t\").rdd\n",
    "dataRDD = rawRDD.map(lambda row: ([None if el is None else int(el) for el in row[1:14]] + row[14:]), int(row[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataRDD.takeSample(withReplacement=False, num=10000)\n",
    "\n",
    "numeric_columns = np.array([pair[0][:13] for pair in sample], dtype=np.float)\n",
    "numeric_df = pd.DataFrame(numeric_columns)\n",
    "\n",
    "category_columns = np.array([pair[0][13:] for pair in sample])\n",
    "category_df = pd.DataFrame(category_columns)\n",
    "\n",
    "labels = np.array([pair[1] for pair in sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Hashing Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple hashing trick...\n",
    "# takes array of features, whose values are a hex string\n",
    "# converts hex string to numerical representation and reduce to N bits\n",
    "# returns a new np.array of reduced feature space\n",
    "def featureHash(data, N):\n",
    "    reducedFeatures = [int(x,16) % (2**N) for x in data]\n",
    "    return np.array(reducedFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash to 16 bits to reduce feature space\n",
    "#hash_mod = [x % (2**16) for x in hash_test]\n",
    "hash_mod = featureHash(category_df[0], 16)\n",
    "#print(hash_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a row of features and the label\n",
    "# leave features 0-13 the same (already integers), covert and hash features \n",
    "def hashTest(features, label, N):\n",
    "    newFeatures = list(features[0:13])\n",
    "    for i in range(14, len(features)):\n",
    "        if features[i] == None:\n",
    "            newFeatures += list([None])\n",
    "        else:\n",
    "            newFeatures += list([int(features[i], 16) % 2**N])\n",
    "    yield (newFeatures, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 1, 5, 0, 1382, 4, 15, 2, 181, 1, 2, None, 2, 27803, 24886, 9156, 15512, 52431, 38328, 46434, 58640, 21764, 40088, 49508, 42486, 59119, 45978, 25319, 30322, 5817, 52681, 10909, 6476, None, 7883, 1156, 13319, 56598], 0), ([2, 0, 44, 1, 102, 8, 2, 2, 4, 1, 1, None, 4, 36, 63461, 19671, 15512, 37605, 64704, 14452, 58640, 58875, 18163, 18918, 1417, 31222, 46541, 15201, 16580, 18032, 52681, 44522, 8734, None, 7883, 16011, 13319, 13909], 0), ([2, 0, 1, 14, 767, 89, 4, 2, 245, 1, 3, 3, 45, 40028, 39030, 57729, 15512, 52431, 1185, 14452, 58640, 58507, 24721, 500, 23087, 14991, 4333, 13400, 56959, 4493, None, None, 50278, 25323, 7883, 15452, None, None], 0), ([None, 893, None, None, 4392, None, 0, 0, 0, None, 0, None, None, 43334, 32360, 55030, 15512, 37605, 26779, 14452, 58640, 17211, 57236, 26570, 63206, 14991, 4644, 26639, 51023, 13570, None, None, 23718, None, 7883, 41802, None, None], 0), ([3, -1, None, 0, 2, 0, 3, 0, 0, 1, 1, None, 0, 41629, 35003, 61357, 15512, 35773, 41557, 14452, 58640, 49463, 44887, 27174, 21420, 59119, 46083, 47796, 51023, 51111, None, None, 20842, None, 18318, 12584, None, None], 0), ([None, -1, None, None, 12824, None, 0, 0, 6, None, 0, None, None, 40179, 60572, 56203, 37705, 39912, 63864, 14452, 58640, 58507, 64551, 47223, 16162, 14991, 39456, 65525, 58265, 21091, None, None, 46864, 29940, 16820, 36625, None, None], 0), ([None, 1, 2, None, 3168, None, 0, 1, 2, None, 0, None, None, 10146, 29392, 48042, 37705, 37605, 65514, 14452, 58640, 58507, 39595, 13624, 215, 14991, 14382, 15817, 58265, 33369, None, None, 9746, None, 53952, 28257, None, None], 0), ([1, 4, 2, 0, 0, 0, 1, 0, 0, 1, 1, None, 0, 43334, 40380, 60048, 16681, 35773, 40180, 46434, 58640, 4094, 40737, 12039, 10851, 14991, 4644, 48583, 30322, 13570, None, None, 41343, None, 18318, 41802, None, None], 1), ([None, 44, 4, 8, 19010, 249, 28, 31, 141, None, 1, None, 8, 21343, 49763, 57729, 15512, 52431, 44274, 14452, 58640, 53070, 49874, 2555, 44544, 31222, 11977, 35626, 30322, 60857, None, None, 49962, None, 18318, 15452, None, None], 0), ([None, 35, None, 1, 33737, 21, 1, 2, 3, None, 1, None, 1, 16549, 31780, 55592, 15512, None, 15644, 14452, 58640, 51347, 57236, 58537, 63206, 14991, 4636, 5241, 31704, 62969, None, None, 64672, None, 18318, 58133, None, None], 0)]\n"
     ]
    }
   ],
   "source": [
    "# convert each categorical column to used hashed data\n",
    "#hashRDD = dataRDD.map(lambda row: hashRDD(row[0], row[1], 16))\n",
    "\n",
    "#hashRDD = dataRDD.map(lambda r: (r[0], r[1]))\n",
    "\n",
    "hashRDD = dataRDD.flatMap(lambda r: hashTest(r[0], r[1], 16))\n",
    "\n",
    "print(hashRDD.take(10))\n",
    "\n",
    "#print(dataRDD.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for \"Homegrown\" Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate_wReg(dataRDD, W, learningRate = 0.1, regType = None, regParam = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update with ridge or lasso regularization.\n",
    "    Args:\n",
    "        dataRDD - tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        learningRate - (float) defaults to 0.1\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient\n",
    "    Returns:\n",
    "        model   - (array) updated coefficients, bias still at index 0\n",
    "    \"\"\"\n",
    "    # augmented data\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    \n",
    "    new_model = None\n",
    "    #################### YOUR CODE HERE ###################\n",
    "    # get the size of data\n",
    "    count = augmentedData.count()\n",
    "    \n",
    "    # gradent descent\n",
    "    grad = augmentedData.map(lambda x: 2.0*x[0].dot((W.dot(x[0]) - x[1]))) \\\n",
    "                        .reduce(lambda a,b: (a + b))\n",
    "    grad /= count\n",
    "    \n",
    "    # ridge\n",
    "    if regType == \"ridge\":\n",
    "        grad[1:] += 2.0*regParam*W[1:]\n",
    "    # lasso\n",
    "    else:\n",
    "        grad[1:] += regParam*np.sign(W[1:])\n",
    "\n",
    "    # update the coefficients\n",
    "    new_model = W - learningRate*grad\n",
    "    \n",
    "    ################## (END) YOUR CODE ####################\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - ridge/lasso gradient descent function\n",
    "def GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 20, learningRate = 0.1,\n",
    "                         regType = None, regParam = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of regularized gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps):  \n",
    "        # update the model\n",
    "        model = GDUpdate_wReg(trainRDD, model, learningRate, regType, regParam)\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(OLSLoss(trainRDD, model))\n",
    "        test_history.append(OLSLoss(testRDD, model))\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
