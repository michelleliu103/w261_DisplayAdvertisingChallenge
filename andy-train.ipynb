{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel, LabeledPoint, RandomForest\n",
    "\n",
    "app_name = \"w261_final_training\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawRDD = spark.read.csv(\"data/train.txt\", header=False, sep=\"\\t\").rdd\n",
    "dataRDD = rawRDD.map(lambda row: ([None if el is None else int(el) for el in row[1:14]] + list(row[14:]), int(row[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data we saved from the EDA. This helps us engineer the features and configure the model\n",
    "\n",
    "frequent_feats = {}\n",
    "\n",
    "with open(\"data/freq_category_counts.csv\") as csvfile:\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        total = int(row[\"total\"])\n",
    "        if total >= 10:\n",
    "            key = \"{}-{}\".format(row[\"col_name\"], row[\"category\"])\n",
    "            frequent_feats[key] = int(row[\"category_id\"])\n",
    "\n",
    "with open(\"data/num_significant_categories.csv\") as csvfile:\n",
    "    num_significant_categories = { row[\"field\"]: int(row[\"count\"]) for row in csv.DictReader(csvfile) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles of Trees\n",
    "\n",
    "The numerical columns are pretty much used directly. Note that NULLs are encoded with the value `-10` (also tried 0 and imputing with the medians). Categorical values are kept if they are in the common categories from the EDA. They were assigned an integer ID, which is in the `frequent_feats` dict. This is used to encode each value. All rare values are converted to a special ID. NULLs are converted to yet another special ID. Note that this adds 2 additional categories from the ones we picked from the EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labeled(pair):\n",
    "    \"\"\"transform input data into the features\"\"\"\n",
    "    row, label = pair\n",
    "    # collect the converted values here\n",
    "    vector = []\n",
    "    \n",
    "    for i, val in enumerate(row):\n",
    "        # if this is an numerical column\n",
    "        if i < 13:\n",
    "            if val is None:\n",
    "                val = -10\n",
    "        # if this is categorical\n",
    "        else:\n",
    "            if val is not None:\n",
    "                key = \"C{}-{}\".format(i - 13, val)\n",
    "                # if its one of our \"common\" values\n",
    "                if key in frequent_feats:\n",
    "                    # look up its ID\n",
    "                    val = frequent_feats[key]\n",
    "                else:\n",
    "                    # give it the special value for RARE\n",
    "                    val = num_significant_categories[\"C\" + str(i - 13)]\n",
    "            else:\n",
    "                # give it the special value for NULL\n",
    "                val = num_significant_categories[\"C\" + str(i - 13)] + 1\n",
    "        vector.append(val)\n",
    "    return LabeledPoint(label, vector)\n",
    "\n",
    "def resample(pair):\n",
    "    \"\"\"sample the positive examples twice to increase their importance\"\"\"\n",
    "    if pair.label == 1:\n",
    "        return [pair, pair]\n",
    "    else:\n",
    "        return [pair]\n",
    "\n",
    "labeledRDD = dataRDD.map(to_labeled)\n",
    "\n",
    "# set model params\n",
    "categoricalFeaturesInfo = { int(feat[1:]) + 13: count + 2 for feat, count in num_significant_categories.items() }\n",
    "maxBins = max(num_significant_categories.values()) + 2\n",
    "trainingData, validationData = labeledRDD.randomSplit([0.9, 0.1])\n",
    "# re-samples the positive class\n",
    "#trainingData = trainingData.flatMap(resample)\n",
    "\n",
    "labels = validationData.map(lambda lp: lp.label).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7239152371342079\n"
     ]
    }
   ],
   "source": [
    "model_gbdt = GradientBoostedTrees.trainClassifier(trainingData,\n",
    "                                                  categoricalFeaturesInfo=categoricalFeaturesInfo,\n",
    "                                                  maxBins=maxBins,\n",
    "                                                  maxDepth=6,\n",
    "                                                  numIterations=10) # how many trees\n",
    "\n",
    "# Evaluate model on test instances and compute validation accuracy\n",
    "predictions_gbdt = model_gbdt.predict(validationData.map(lambda x: x.features))\n",
    "#labelsAndPredictions = validationData.map(lambda lp: lp.label).zip(predictions)\n",
    "#testErr = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(validationData.count())\n",
    "\n",
    "preds_gbdt = predictions_gbdt.collect()\n",
    "accuracy_gbdt = np.mean(np.array(labels) == np.array(preds_gbdt))\n",
    "print('accuracy = ' + str(accuracy_gbdt))\n",
    "\n",
    "model_name = \"models/gbdt-model\"\n",
    "# save the model\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_gbdt.save(sc, model_name)\n",
    "\n",
    "# how to load the model again, tho not necessary in this file\n",
    "#sameModel = GradientBoostedTreesModel.load(sc, model_name)\n",
    "#print(sameModel.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.80      0.81      7402\n",
      "        1.0       0.46      0.50      0.48      2508\n",
      "\n",
      "avg / total       0.73      0.72      0.73      9910\n",
      "\n",
      "[[5918 1484]\n",
      " [1252 1256]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_gbdt))\n",
    "print(confusion_matrix(labels, preds_gbdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some config I tried so far:\n",
    "\n",
    "|max depth|num iterations|top n categories|re-sampling ratio neg:pos|acc|1-recall|F1 score|note|\n",
    "|---------|--------------|----------------|-------------------------|---|--------|--|----|\n",
    "|3|5|200000|1:1|-|-|-|the model fails to train due to out-of-memory error|\n",
    "|3|5|50|1:1|75%|9%|67%|basically predicts all 0's|\n",
    "|6|10|50|1:1|76%|21%|71%|Model getting much bigger though. Takes about 2.5 hours to train full data on my laptop|\n",
    "|6|10|50|1:2|72%|50%|73%|trades off accuracy for recall|\n",
    "|6|15|50|1:2|72%|50%|73%|no improvement|\n",
    "|12|10|50|1:2|70%|48%|71%|actually gets worse on all metrics|\n",
    "|6|10|75|1:2|72%|50%|73%|no improvement|\n",
    "|4|10|75|1:2|71%|49%|72%|slight decrease on all metrics|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.6859737638748739\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForest.trainClassifier(trainingData,\n",
    "                                        categoricalFeaturesInfo=categoricalFeaturesInfo,\n",
    "                                        maxBins=maxBins,\n",
    "                                        numClasses=2,\n",
    "                                        maxDepth=15,\n",
    "                                        numTrees=10)\n",
    "\n",
    "# Evaluate model on test instances and compute validation accuracy\n",
    "predictions_rf = model_rf.predict(validationData.map(lambda x: x.features))\n",
    "\n",
    "preds_rf = predictions_rf.collect()\n",
    "accuracy_rf = np.mean(np.array(labels) == np.array(preds_rf))\n",
    "print('accuracy = ' + str(accuracy_rf))\n",
    "\n",
    "model_name = \"models/rf-model\"\n",
    "# save the model\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_rf.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.85      0.71      0.77      7402\n",
      "        1.0       0.42      0.62      0.50      2508\n",
      "\n",
      "avg / total       0.74      0.69      0.70      9910\n",
      "\n",
      "[[5251 2151]\n",
      " [ 961 1547]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_rf))\n",
    "print(confusion_matrix(labels, preds_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some config I tried so far:\n",
    "\n",
    "|max depth|num trees|top n categories|re-sampling ratio neg:pos|acc|1-recall|F1 score|note|\n",
    "|---------|---------|----------------|-------------------------|---|--------|--|----|\n",
    "|6|10|50|1:1|75%|19%|70%|almost as good as GBDT|\n",
    "|10|10|50|1:1|75%|23%|71%|benefits from a little more depth|\n",
    "|10|15|50|1:1|75%|19%|70%|more trees is worse|\n",
    "|15|15|50|1:1|75%|27%|72%|but more depth continues to benefit|\n",
    "|15|10|50|1:1|75%|31%|73%|and dialing back # of trees is even better|\n",
    "|15|5|50|1:1|75%|29%|72%|but too few trees is bad|\n",
    "|30|5|50|1:1|72%|42%|71%|doubling the depth gets very good recall|\n",
    "|30|15|50|1:1|74%|34%|72%|adding more trees shifts the trade-off|\n",
    "|15|10|50|1:2|68%|62%|70%|resampling gets way better recall|\n",
    "|15|15|50|1:2|70%|60%|71%||\n",
    "|20|15|50|1:2|69%|59%|70%||\n",
    "|10|10|50|1:2|70%|55%|71%||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start desperately throwing models at this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_encoded(pair):\n",
    "    \"\"\"transform input data into the features, but use one-hot encoding for the categorical variables\"\"\"\n",
    "    row, label = pair\n",
    "    # collect the converted values here\n",
    "    vector = []\n",
    "    \n",
    "    for i, val in enumerate(row):\n",
    "        # if this is an numerical column\n",
    "        if i < 13:\n",
    "            if val is None:\n",
    "                val = 0\n",
    "            val = np.max([0, val])\n",
    "            vector.append(val)\n",
    "        # if this is categorical\n",
    "        else:\n",
    "            feat_len = num_significant_categories[\"C\" + str(i - 13)]\n",
    "            one_hot = np.zeros(feat_len + 2)\n",
    "            if val is not None:\n",
    "                key = \"C{}-{}\".format(i - 13, val)\n",
    "                # if its one of our \"common\" values\n",
    "                if key in frequent_feats:\n",
    "                    # look up its ID\n",
    "                    one_hot[frequent_feats[key]] = 1\n",
    "                else:\n",
    "                    # give it the special value for RARE\n",
    "                    one_hot[feat_len] = 1\n",
    "            else:\n",
    "                # give it the special value for NULL\n",
    "                one_hot[feat_len + 1] = 1\n",
    "            vector.extend(one_hot)\n",
    "    return LabeledPoint(label, vector)\n",
    "\n",
    "def resample(pair):\n",
    "    \"\"\"sample the positive examples twice to increase their importance\"\"\"\n",
    "    if pair.label == 1:\n",
    "        return [pair, pair]\n",
    "    else:\n",
    "        return [pair]\n",
    "\n",
    "encodedRDD = dataRDD.map(to_encoded)\n",
    "trainingData, validationData = encodedRDD.randomSplit([0.9, 0.1])\n",
    "labels = validationData.map(lambda lp: lp.label).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.4633343310386112\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model_nb = NaiveBayes.train(trainingData, lambda_=1.0)\n",
    "predictions_nb = model_nb.predict(validationData.map(lambda x: x.features))\n",
    "\n",
    "preds_nb = predictions_nb.collect()\n",
    "accuracy_nb = np.mean(np.array(labels) == np.array(preds_nb))\n",
    "print(\"accuracy = \" + str(accuracy_nb))\n",
    "\n",
    "model_name = \"models/nb-model\"\n",
    "# save the model\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_nb.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.36      0.50      7432\n",
      "        1.0       0.29      0.75      0.42      2591\n",
      "\n",
      "avg / total       0.67      0.46      0.48     10023\n",
      "\n",
      "[[2710 4722]\n",
      " [ 657 1934]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_nb))\n",
    "print(confusion_matrix(labels, preds_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7249326548937444\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "model_lr = LogisticRegressionWithSGD.train(trainingData)\n",
    "predictions_lr = model_lr.predict(validationData.map(lambda x: x.features))\n",
    "\n",
    "preds_lr = predictions_lr.collect()\n",
    "accuracy_lr = np.mean(np.array(labels) == np.array(preds_lr))\n",
    "print(\"accuracy = \" + str(accuracy_lr))\n",
    "\n",
    "model_name = \"models/lr-model\"\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_lr.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.92      0.83      7432\n",
      "        1.0       0.42      0.16      0.23      2591\n",
      "\n",
      "avg / total       0.67      0.72      0.68     10023\n",
      "\n",
      "[[6851  581]\n",
      " [2176  415]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_lr))\n",
    "print(confusion_matrix(labels, preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7247331138381722\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model_svm = SVMWithSGD.train(trainingData)\n",
    "predictions_svm = model_svm.predict(validationData.map(lambda x: x.features))\n",
    "\n",
    "preds_svm = predictions_svm.collect()\n",
    "accuracy_svm = np.mean(np.array(labels) == np.array(preds_svm))\n",
    "print(\"accuracy = \" + str(accuracy_svm))\n",
    "\n",
    "model_name = \"models/svm-model\"\n",
    "!rm -rf /media/notebooks/{model_name}\n",
    "model_svm.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.92      0.83      7432\n",
      "        1.0       0.42      0.16      0.23      2591\n",
      "\n",
      "avg / total       0.67      0.72      0.68     10023\n",
      "\n",
      "[[6849  583]\n",
      " [2176  415]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds_svm))\n",
    "print(confusion_matrix(labels, preds_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
